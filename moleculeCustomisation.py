# -*- coding: utf-8 -*-
"""Molecule.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t1pMh08ioBWq6Hi5EgYQHMMiuBhxG2c5
"""


import torch
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset
import os
import pandas as pd
import re
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name



#!pip install -q transformers

#pip install transformers seqeval

#from transformers import BertModel, BertTokenizer
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from transformers.models.bert.modeling_bert import BertModel
from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForTokenClassification, BertTokenizerFast, EvalPrediction, BertForTokenClassification, BertTokenizer

from transformers import BertConfig, AutoConfig
from transformers.models.bert.modeling_bert import BertPreTrainedModel

from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score


from transformers.modeling_outputs import TokenClassifierOutput

from transformers import pipeline

#!huggingface-cli login
from huggingface_hub import list_repo_files




def main():
    #huggingface-cli login
    
    df = pd.read_csv('protein.csv')

    data = df.loc[df['len']>= 50]


    #model_name = "Rostlab/prot_bert"
    #model_name = "https://huggingface.co/Rostlab/prot_bert/tree/main"
    #model_config_address = "https://huggingface.co/Rostlab/prot_bert/tree/main"
    model_name = 'Rostlab/prot_bert_bfd'
    #model_name = 'https://huggingface.co/Rostlab/prot_bert_bfd/tree/main'
    
    #model_name = "Rostlab/prot_bert_bfd_ss3"
    
    #model_checkpoint = get_full_repo_name(model_name)
    #print(model_checkpoint)
    #reader = pipeline( model=model_name)
    #print(reader)
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(device)

    train_seqs = [ list(seq) for seq in data['seq']]
    train_labels = [ list(label) for label in data['sst3']]

    print(train_seqs[0][10:30], train_labels[0][10:30],  sep='\n')

    seq_tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=False)
    #seq_tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)
    x = data.seq.values
    y = data.sst3.values
    # set aside 20% of train and test data for evaluation
    X_train, X_test, y_train, y_test = train_test_split(x, y,
        test_size=0.2, shuffle = True, random_state = 8)

    # Use the same function above for the validation set
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, 
        test_size=0.1,shuffle = True, random_state= 8) # 0.25 x 0.8 = 0.2
    
    
    train_seqs = [ list(seq) for seq in X_train]
    train_labels = [ list(label) for label in y_train]
    

    val_seqs = [ list(seq) for seq in X_val]
    val_labels = [ list(label) for label in y_val]

    test_seqs = [ list(seq) for seq in X_test]
    test_labels = [ list(label) for label in y_test]
    
    """*6*. Tokenize sequences"""
    train_seqs_encodings = seq_tokenizer(train_seqs, is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding=True)
    val_seqs_encodings = seq_tokenizer(val_seqs, is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding=True)
    test_seqs_encodings = seq_tokenizer(test_seqs, is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding=True)
    
    
    # train_seqs_encodings = seq_tokenizer(train_seqs, is_split_into_words=True,  truncation=True, padding=True)
    # val_seqs_encodings = seq_tokenizer(val_seqs, is_split_into_words=True,  truncation=True, padding=True)
    # test_seqs_encodings = seq_tokenizer(test_seqs, is_split_into_words=True, truncation=True, padding=True)

#     #"""7. Tokenize labels"""

# # Consider each label as a tag for each token
    unique_tags = set(tag for doc in train_labels for tag in doc)
    unique_tags  = sorted(list(unique_tags))  # make the order of the labels unchanged
    tag2id = {tag: id for id, tag in enumerate(unique_tags)}
    id2tag = {id: tag for tag, id in tag2id.items()}
    print("id to tag is: ", id2tag)
    print("tag to id is: ", tag2id)

    def encode_tags(tags, encodings):
        labels = [[tag2id[tag] for tag in doc] for doc in tags]
        encoded_labels = []
        for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):
        # create an empty array of -100
            doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100
            arr_offset = np.array(doc_offset)

        # set labels whose first offset position is 0 and the second is not 0
            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels
            encoded_labels.append(doc_enc_labels.tolist())

        return encoded_labels

    train_labels_encodings = encode_tags(train_labels, train_seqs_encodings)
    val_labels_encodings = encode_tags(val_labels, val_seqs_encodings)
    test_labels_encodings = encode_tags(test_labels, test_seqs_encodings)


#     #"""9. Create SS3 Dataset"""

    class SS3Dataset(Dataset):
        def __init__(self, encodings, labels):
            self.encodings = encodings
            self.labels = labels

        def __getitem__(self, idx):
            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
            item['labels'] = torch.tensor(self.labels[idx])
            return item

        def __len__(self):
            return len(self.labels)

# # we don't want to pass this to the model
    _ = train_seqs_encodings.pop("offset_mapping")
    _ = val_seqs_encodings.pop("offset_mapping")
    _ = test_seqs_encodings.pop("offset_mapping")

    train_dataset = SS3Dataset(train_seqs_encodings, train_labels_encodings)
    val_dataset = SS3Dataset(val_seqs_encodings, val_labels_encodings)
    test_dataset = SS3Dataset(test_seqs_encodings, test_labels_encodings)
    
    train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = 1 )

# #"""10. Define the evaluation metrics"""

    def align_predictions(predictions: np.ndarray, label_ids: np.ndarray):
        
        preds = np.argmax(predictions, axis=2)

        batch_size, seq_len = preds.shape

        out_label_list = [[] for _ in range(batch_size)]
        preds_list = [[] for _ in range(batch_size)]

        for i in range(batch_size):
            for j in range(seq_len):
                if label_ids[i, j] != torch.nn.CrossEntropyLoss().ignore_index:
                    out_label_list[i].append(id2tag[label_ids[i][j]])
                    preds_list[i].append(id2tag[preds[i][j]])

        return preds_list, out_label_list

    def compute_metrics(p: EvalPrediction):
        preds_list, out_label_list = align_predictions(p.predictions, p.label_ids)
        return {
        "accuracy": accuracy_score(out_label_list, preds_list),
        "precision": precision_score(out_label_list, preds_list),
        "recall": recall_score(out_label_list, preds_list),
        "f1": f1_score(out_label_list, preds_list),
        }


#      #https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_ner.ipynb
#      #https://discuss.huggingface.co/t/what-is-the-purpose-of-the-additional-dense-layer-in-classification-heads/526
#      #very importatn https://discuss.huggingface.co/t/how-do-i-change-the-classification-head-of-a-model/4720/11
#      #https://www.oreilly.com/library/view/natural-language-processing/9781098103231/ch04.html
#      #https://stackoverflow.com/questions/66148641/changing-config-and-loading-hugging-face-model-fine-tuned-on-a-downstream-task
#      #https://github.com/multicom-toolbox/DNSS2/blob/master/scripts/test_dnss.py
        #https://huggingface.co/Rostlab/prot_bert_bfd/blame/main/README.md
        

    
    class BertForTokenClassification(BertPreTrainedModel):
        
        config_class = BertConfig
        
        def __init__(self,config):
            
            super().__init__(config)
            
            self.num_labels = 3
             # Load model body
            self.model = BertModel.from_pretrained(config, add_pooling_layer=False, local_files_only = False )
                                                    #use_auth_token=True)
            #self.model["use_auth_token"] = model_kwargs.get("use_auth_token", 
                                                                #use_auth_token)
            print("the output of the original model is: ", self.model)
                                                                
            # Set up token classification head
            self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)
            #self.linear1 = torch.nn.Linear(1024, 1024)
            #self.relu = torch.nn.ReLU()
            #self.dropout2 = torch.nn.Dropout(0.3)
            self.linear2 = torch.nn.Linear(config.hidden_size,3)
            # Load and initialize weights
            self.init_weights()
            
        def forward(self, ids = None, mask=None, labels= None, **kwargs):
            
            # Use model body to get encoder representations
            outputs  = self.model(input_ids=ids, attention_mask = mask, **kwargs)
            
            # Apply classifier to encoder representation
            sequence_output = self.dropout(outputs[0])
            logits = self.classifier(sequence_output)
            
            # Apply classifier to encoder representation
            # output_dropout1= self.dropout(output_l1[0])
            # output_linear1 = self.linear1(output_dropout1)
            # output_relu = self.relu(output_linear1)
            # output_dropout = self.dropout2(output_relu)
            # logits = self.linear2(output_dropout)
            
            
            # Calculate losses
            loss = None
            if labels is not None:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
                  # Return model output object
            return TokenClassifierOutput(loss=loss, logits=logits,
                                     hidden_states=outputs.hidden_states,
                                     attentions=outputs.attentions)
            
    
    
    model_name =  'Rostlab/prot_bert_bfd/'    
   
    model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path = model_name,
                                         num_labels=3,
                                         id2label=id2tag,
                                        label2id=tag2id,
                                        gradient_checkpointing=False, local_files_only = False )
       
    def model_init():
        
        return (BertForTokenClassification.from_pretrained(model_name, config = model_config ).to(device))
    
    
    def tag_text(text, tags, model, tokenizer):
    # Get tokens with special characters
        tokens = tokenizer(text).tokens()
    # Encode the sequence into IDs
        input_ids = seq_tokenizer(text, return_tensors="pt").input_ids.to(device)
    # Get predictions as distribution over 7 possible classes
        outputs = model(inputs)[0]
    # Take argmax to get most likely class per token
        predictions = torch.argmax(outputs, dim=2)
    # Convert to DataFrame
        preds = [tags.names[p] for p in predictions[0].cpu().numpy()]
        return pd.DataFrame([tokens, preds], index=["Tokens", "Tags"])
    
    # for epoch in range(0, 3):
        
    #     total_loss = 0
        
    #     for step, batch in enumerate(train_dataloader):
            
    #         b_input_ids = batch['input_ids'].to(device, dtype = torch.long)
    #         b_attention_mask = batch['attention_mask'].to(device, dtype = torch.long)
    #         b_labels = batch['labels'].to(device, dtype = torch.long)
    #         print(b_input_ids.shape)
    #         print(b_attention_mask.shape)
    #         print(b_labels.shape)
    #         print(b_input_ids.size())
    #         print(b_attention_mask.size())
    #         print(b_labels.size())
    #         print("the output of the model is:", model(b_input_ids, b_attention_mask,  b_labels))
            
    #         loss = model(b_input_ids, b_attention_mask, b_labels.unsqueeze(dim=0))[0]
            
    #         # if step ==500:
                
    #         #     print(f'Epoch: {epoch} , Loss: {loss.item()}')
                
    #         # optimizer.zero_grad()
    #         # loss.backward()
    #         # optimizer.step()
            
    
    training_args = TrainingArguments(
    output_dir='./results/customModel',          # output directory
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=1,   # batch size per device during training
    per_device_eval_batch_size=8,   # batch size for evaluation
    warmup_steps=200,                # number of warmup steps for learning rate scheduler
    learning_rate=3e-05,             # learning rate
    weight_decay=0.0,                # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=200,               # How often to print logs
    do_train=True,                   # Perform training
    do_eval=True,                    # Perform evaluation
    evaluation_strategy="epoch",     # evalute after each epoch
    save_strategy='epoch',
    gradient_accumulation_steps=32,  # total number of steps before back propagation
    fp16=True,                       # Use mixed precision
    fp16_opt_level="02",             # mixed precision mode
    run_name="ProBert-BFD-SS3-customModel",      # experiment name
    seed=3,                         # Seed for experiment reproducibility
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    greater_is_better=True,
    #use_auth_token=True
    #hub_token = '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'

)
    trainer = Trainer(
    model_init=model_init,                # the instantiated ðŸ¤— Transformers model to be trained
    args=training_args,                   # training arguments, defined above
    train_dataset=train_dataset,          # training dataset
    eval_dataset=val_dataset,             # evaluation dataset
    compute_metrics = compute_metrics,    # evaluation metrics
    )

    trainer.train()

#"""13. Make predictions and evaluate"""

    predictions, label_ids, metrics = trainer.predict(test_dataset)
    #print(metrics)
    print(compute_metrics(trainer.predict(test_dataset)))


    idx = 0
    sample_ground_truth = " ".join([id2tag[int(tag)] for tag in test_dataset[idx]['labels'][test_dataset[idx]['labels'] != torch.nn.CrossEntropyLoss().ignore_index]])
    sample_predictions =  " ".join([id2tag[int(tag)] for tag in np.argmax(predictions[idx], axis=1)[np.argmax(predictions[idx], axis=1) != torch.nn.CrossEntropyLoss().ignore_index]])
    sample_sequence = seq_tokenizer.decode(list(test_dataset[idx]['input_ids']), skip_special_tokens=True)
    print("Sequence       : {} \nGround Truth is: {}\nprediction is  : {}".format(sample_sequence,
                                                                      sample_ground_truth,
                                                                      # Remove the first token on prediction becuase its CLS token
                                                                      # and only show up to the input length
                                                                      sample_predictions[2:len(sample_sequence)+2]))
   


    #save the model

    trainer.save_model('prot_bert_bfd_ss3_customModel/')

    seq_tokenizer.save_pretrained('prot_bert_bfd_ss3_customModel/')
    
     #tag_text(sample_ground_truth, tags, trainer.model, seq_tokenizer)
    
if __name__ == "__main__":
    main()